PDFs mit LLMs

ğŸ“š Beschreibung

Dieses Projekt enthÃ¤lt ein Chatbot-Frontend, das mit React entwickelt wurde, sowie einen Python-Server zur Verarbeitung der Anfragen. ZusÃ¤tzlich umfasst es eine grafische BenutzeroberflÃ¤che zur Extraktion von Informationen aus PDF-Anmeldeformularen zur Bachelorarbeit. Mithilfe des Llama-Algorithmus werden relevante Informationen wie Name, Vorname, Matrikelnummer, Thema und Betreuer extrahiert und als JSON-Datei gespeichert.

Die BenutzeroberflÃ¤che soll zudem eine Auswahl an vorgefertigten Fragen bereitstellen, um verschiedene TestfÃ¤lle zu ermÃ¶glichen. ZusÃ¤tzlich kÃ¶nnen Benutzer zwischen verschiedenen Modellen wÃ¤hlen und Parameter individuell anpassen.

ğŸ‘¨â€ğŸ’» Voraussetzungen

Stelle sicher, dass du folgende Software und Bibliotheken installiert hast:

Python 3.8+

pip (Python-Paketmanager)

Ollama (zur AusfÃ¼hrung der LLMs)

ChromaDB (Vektordatenbank zur Speicherung der Embeddings)

Node.js & npm (fÃ¼r das React-Frontend)

ğŸ“¦ Installation

1ï¸âƒ£ Python-Umgebung einrichten

Erstelle eine virtuelle Umgebung (optional):

python -m venv venv

venv\Scripts\activate  # FÃ¼r Windows

2ï¸âƒ£ Erforderliche AbhÃ¤ngigkeiten installieren

pip install -r requirements.txt

3ï¸âƒ£ Ollama installieren und starten

Lade Ollama herunter und installiere es. Starte anschlieÃŸend den Server:

ollama serve

ğŸ“‚ Nutzung

1ï¸âƒ£ PDFs in ChromaDB laden

FÃ¼hre das Skript aus:

python Embeddings.py

Speichere deine PDFs im Ordner pdf_files/, um die Inhalte in Embeddings umzuwandeln und in der Chroma-Datenbank zu speichern.

2ï¸âƒ£ Informationen extrahieren

Starte das Hauptskript zur Extraktion der Informationen:

python extract_info.py

3ï¸âƒ£ Ergebnisse Ã¼berprÃ¼fen

Die extrahierten Daten werden als JSON-Dateien im output_json/ Ordner gespeichert.

ğŸ† Start von React-App

1ï¸âƒ£ React-Frontend fÃ¼r Produktion bauen

Navigiere in den react-Ordner und fÃ¼hre den folgenden Befehl aus, um ein Produktions-Build zu erstellen:

cd chatbot-frontend

npm install

npm run build

Dadurch wird ein build/-Ordner erstellt, der vom Server verwendet wird. Zudem ist React als Proxy in der package.json-Datei definiert, sodass API-Anfragen wÃ¤hrend der Entwicklung direkt an den Flask-Server weitergeleitet werden (nur lokal). Das Routing Ã¼bernimmt React vollstÃ¤ndig, wÃ¤hrend die Flask-App immer nur die index.html-Seite zurÃ¼ckgibt(SPA).

2ï¸âƒ£ Python-Server starten

Wechsle zurÃ¼ck zum Hauptverzeichnis und starte den Server mit:

cd ..

python server.py

Dadurch wird der Server gestartet und das Frontend aus dem build/-Ordner unter dem Port 5000 ausgeliefert.

ğŸ›  Fehlerbehebung

Falls npm run build nicht funktioniert:

Stelle sicher, dass Node.js und npm installiert sind (node -v und npm -v prÃ¼fen).

Falls Pakete fehlen, fÃ¼hre npm install im react-Ordner aus.

Falls python server.py nicht funktioniert:

Stelle sicher, dass Python installiert ist (python --version prÃ¼fen).

Falls AbhÃ¤ngigkeiten fehlen, installiere sie mit pip install -r requirements.txt.

Falls ollama serve nicht lÃ¤uft:

Starte den Server mit ollama serve

Falls ChromaDB Fehler auftreten:

LÃ¶sche den chroma_db/ Ordner und starte python Embeddings.py neu

Falls Modelle fehlen:

Stelle sicher, dass llama3.1:8b oder deepseek-r1:14b in Ollama verfÃ¼gbar sind

ğŸ† Fazit

Nach dem Build kannst du die Anwendung einfach mit python server.py starten, und dein Chatbot lÃ¤uft im Produktionsmodus! ZusÃ¤tzlich kannst du PDFs analysieren und extrahierte Daten in JSON speichern. ğŸš€




runi e server w jawek bhy ye zzzzz



